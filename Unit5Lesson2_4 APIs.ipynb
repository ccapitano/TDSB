{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at Amazon's robots.txt file (or Twitter's, or Facebook's), you may be surprised to see them prohibit or severely restrict scraping.  Aren't there a lot of projects online using Twitter data?  And how dare they keep all that delicious, delicious information to themselves?  But before you start setting `'ROBOTSTXT_OBEY' = False`, read on!\n",
    "\n",
    "Most of The Big Websites (Google, Facebook, Twitter, etc) have APIs that allow you to access their information programmatically without using webpages.  This is good for both you and the website.  With an API, you can ask the server to send you only the specific information you want, without having to retrieve, filter out, and discard the CSS, HTML, PHP, and other code from the website.  This minimizes demand on the server and speeds up your task.  \n",
    "\n",
    "APIs typically include their own throttling to keep you from overloading the server, usually done by limiting the number of server requests per hour to a certain number.  \n",
    "\n",
    "To access an API, you will usually need an API key or token that uniquely identifies you.  This lets the company or service providing the API keep an eye on your usage and track what you are doing.  Different API keys can also be associated with different levels of authorization and access, so they work as a data security measure.  Keys or tokens may also be set to expire after a certain amount of time or number of uses.\n",
    "\n",
    "## Anatomy of an API\n",
    "\n",
    "*Access*- You request a key.  Your program provides the key with each API call, and it determines what your program can do in the API.  \n",
    "*Requests*- Your program requests the data you want with a call to the API.  The request will be made up of a method (type of query, using language defined by the API) and parameters (refine the query).  \n",
    "*Response*- The data returned by the API, usually in a common format such as JSON that your program can parse.  \n",
    "\n",
    "The specific syntax for each of these elements, and the format of the response, will vary from API to API.  In addition, APIs vary widely in their level of documentation and ease of use.  Before diving too deeply into an API-scraping project, do some judicious googling and if you see a lot of posts [like this one](https://mollyrocket.com/casey/stream_0029.html) consider going elsewhere.  Not all websites put their APIs front-and-center (did you know there are APIs for [NASA](https://api.nasa.gov/), [Marvel Comics](http://developer.marvel.com/), and [Star Wars](https://swapi.co/)?) so google will be your friend there as well.\n",
    "\n",
    "## Basics of API Queries: Wikipedia's API\n",
    "\n",
    "The process of using an API sounds a lot like scraping (make request, get response), but with an occasional added authorization layer.  Scrapy can handle authorization, so we can use it to access APIs too.\n",
    "\n",
    "That said, the first API we'll pull from is [Wikipedia's](https://www.mediawiki.org/wiki/API:Main_page), which doesn't require an authorization key.  Aside from needing to master the API's language, you'll find that using scrapy with an API is very similar to using scrapy on a website.\n",
    "\n",
    "We want to know what other entries on Wikipedia link to the [Monty Python](https://en.wikipedia.org/wiki/Monty_Python) page.  To do this, we can build a query using the [Wikipedia API Sandbox](https://en.wikipedia.org/wiki/Special:ApiSandbox).  Someone who is comfortable with the MediaWiki API syntax wouldn't need to use the sandbox, but for beginners it is very handy.  Note that API queries are nothing like SQL queries in syntax, despite their shared name.\n",
    "\n",
    "The query we will use looks like this:\n",
    "`https://en.wikipedia.org/w/api.php?action=query&format=xml&prop=linkshere&titles=Monty_Python&lhprop=title%7Credirect`\n",
    "\n",
    "Let's break that down into it's components:\n",
    "\n",
    "* `w/api.php`\n",
    "    * Tells the server that we are using the API to pull info, rather than scraping the raw pages.  \n",
    "    \n",
    "* `action=query`   \n",
    "    * We want information from the API (as opposed to changing information in the API)  \n",
    "    \n",
    "* `format=xml`  \n",
    "    * Format the return in xml- then we will parse it with xpath  \n",
    "    \n",
    "* `prop=linkshere`  \n",
    "    * We are interested in which pages link to our target page \n",
    "    \n",
    "* `titles=Monty_Python`  \n",
    "    * The target page is the Monty Python page.  Note that we used the exact name of the wikipedia page (Monty_Python).  \n",
    "    \n",
    "* `lhprop=title`  \n",
    "    * From those links, we want the title of each page  \n",
    "    \n",
    "* `redirect`  \n",
    "    * We also want to know if that link is a redirect  \n",
    "    \n",
    "\n",
    "The syntax of the MediaWiki API is based on php, thus the inclusion of `?` and `&` in the query.\n",
    "\n",
    "For most of the query elements, we could have passed multiple arguments.  For example, we could request the URL as well as the title of the linking pages, or asked for all the pages that link to Monty_Python and to Monty_Python's_Flying_Circus.  \n",
    "\n",
    "A query like this highlights why APIs are so handy.  Without an API, to find out the name of every page on Wikipedia that links to the Monty Python page we would have to scrape every single one of the 5,000,000+ articles in the English-language Wikipedia.  \n",
    "\n",
    "If you haven't done so already, click on the query link above and see what it returns.\n",
    "\n",
    "\n",
    "\n",
    "## Why use Scrapy for API calls\n",
    "\n",
    "For some API calls, scrapy would be overkill.  If you know that your query can be answered in one response, then you don't need scrapy- you can use the `requests` library to make your API call and a library like `lxml` to parse the return.\n",
    "\n",
    "The Wikipedia API, however, will only return ten items at a time in response to a query.  This sort of limitation is common to APIs to avoid overwhelming the server.  We can use scrapy to iterate over query results the same way that we iterated over the pages of the EverydaySexism website. \n",
    "\n",
    "Let's see the Wikipedia API and scrapy in action:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 100 links extracted!\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "class WikiSpider(scrapy.Spider):\n",
    "    name = \"WS\"\n",
    "    \n",
    "    # Here is where we insert our API call.\n",
    "    start_urls = [\n",
    "        'https://en.wikipedia.org/w/api.php?action=query&format=xml&prop=linkshere&titles=Monty_Python&lhprop=title%7Credirect'\n",
    "        ]\n",
    "\n",
    "    # Identifying the information we want from the query response and extracting it using xpath.\n",
    "    def parse(self, response):\n",
    "        for item in response.xpath('//lh'):\n",
    "            # The ns code identifies the type of page the link comes from.  '0' means it is a Wikipedia entry.\n",
    "            # Other codes indicate links from 'Talk' pages, etc.  Since we are only interested in entries, we filter:\n",
    "            if item.xpath('@ns').extract_first() == '0':\n",
    "                yield {\n",
    "                    'title': item.xpath('@title').extract_first() \n",
    "                    }\n",
    "        # Getting the information needed to continue to the next ten entries.\n",
    "        next_page = response.xpath('continue/@lhcontinue').extract_first()\n",
    "        \n",
    "        # Recursively calling the spider to process the next ten entries, if they exist.\n",
    "        if next_page is not None:\n",
    "            next_page = '{}&lhcontinue={}'.format(self.start_urls[0],next_page)\n",
    "            yield scrapy.Request(next_page, callback=self.parse)\n",
    "            \n",
    "    \n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',\n",
    "    'FEED_URI': 'PythonLinks.json',\n",
    "    # Note that because we are doing API queries, the robots.txt file doesn't apply to us.\n",
    "    'ROBOTSTXT_OBEY': False,\n",
    "    'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "    'AUTOTHROTTLE_ENABLED': True,\n",
    "    'HTTPCACHE_ENABLED': True,\n",
    "    'LOG_ENABLED': False,\n",
    "    # We use CLOSESPIDER_PAGECOUNT to limit our scraper to the first 100 links.    \n",
    "    'CLOSESPIDER_PAGECOUNT' : 10\n",
    "})\n",
    "                                         \n",
    "\n",
    "# Starting the crawler with our spider.\n",
    "process.crawl(WikiSpider)\n",
    "process.start()\n",
    "print('First 100 links extracted!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(92, 1)\n",
      "                        title\n",
      "87               Hans Moleman\n",
      "88              Ripping Yarns\n",
      "89  List of British comedians\n",
      "90         Wensleydale cheese\n",
      "91              Art Garfunkel\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Checking whether we got data \n",
    "\n",
    "Monty=pd.read_json('https://tf-assets-prod.s3.amazonaws.com/tf-curric/data-science/PythonLinks.json', orient='records')\n",
    "print(Monty.shape)\n",
    "print(Monty.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap up\n",
    "\n",
    "Our API call was successful.  While we examined 100 links, we only saved 92 (the others weren't links from entry pages).  \n",
    "\n",
    "We've barely scraped (pun intended) the surface of what scrapy and APIs can do.  Scrapy has changed a lot in the years since its debut, so when googling make sure the answers you see are from 2015 at the latest-- otherwise you'll likely not be able to use the code.  \n",
    "\n",
    "Back to the issue of authorization keys- often the key is simply included in the query string as an additional arguments.  In other cases, if you need your scraper to be able to enter a key or login information into a form, scrapy [has you covered](http://stackoverflow.com/questions/30102199/form-authentication-login-a-site-using-scrapy).  \n",
    "\n",
    "There's a lot of fun to be had in scraping and APIs-- it's a way to feel like you're getting a lot of information with very little effort!  Beware, however.  You're not getting information at all.  Scraping gives you *data*, an undifferentiated mess of bytes with no compelling meaning on its own.  Think of that list of Wiki entries that link to Monty Python.  It's cool that we could get it, but what does it mean?  Your job as a data scientist is to convert *data* to *information*-- something people can use to make decisions or understand the world.  Modeling data to get information is hard but worthwhile work, and its those kinds of projects that will really build your portfolio as you go on the market.  \n",
    "\n",
    "That said, scraping up some original data can provide the *foundation* for an interesting and original final project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "Do a little scraping or API-calling of your own.  Pick a new website and see what you can get out of it.  Expect that you'll run into bugs and blind alleys, and rely on your mentor to help you get through.  \n",
    "\n",
    "Formally, your goal is to write a scraper that will:\n",
    "\n",
    "1) Return specific pieces of information (rather than just downloading a whole page)  \n",
    "2) Iterate over multiple pages/queries  \n",
    "3) Save the data to your computer  \n",
    "\n",
    "Once you have your data, compute some statistical summaries and/or visualizations that give you some new insights into your scraping topic of interest.  Write up a report from scraping code to summary and share it with your mentor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-11 15:00:12 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: scrapybot)\n",
      "2019-09-11 15:00:12 [scrapy.utils.log] INFO: Versions: lxml 4.3.2.0, libxml2 2.9.9, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.3 (default, Mar 27 2019, 17:13:21) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1b  26 Feb 2019), cryptography 2.6.1, Platform Windows-10-10.0.17763-SP0\n",
      "2019-09-11 15:00:12 [scrapy.crawler] INFO: Overridden settings: {}\n",
      "2019-09-11 15:00:12 [scrapy.extensions.telnet] INFO: Telnet Password: aaaef802f78a372c\n",
      "2019-09-11 15:00:12 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2019-09-11 15:00:12 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2019-09-11 15:00:12 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2019-09-11 15:00:12 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2019-09-11 15:00:12 [scrapy.core.engine] INFO: Spider opened\n",
      "2019-09-11 15:00:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2019-09-11 15:00:12 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2019-09-11 15:00:12 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://misuse.ncbi.nlm.nih.gov/error/abuse.shtml> from <GET https://eutils.ncbi.nlm.nih.gov/entrez/eutils>\n",
      "2019-09-11 15:00:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://misuse.ncbi.nlm.nih.gov/error/abuse.shtml> (referer: None)\n",
      "2019-09-11 15:00:13 [scrapy.core.scraper] ERROR: Spider error processing <GET https://misuse.ncbi.nlm.nih.gov/error/abuse.shtml> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\twisted\\internet\\defer.py\", line 654, in _runCallbacks\n",
      "    current.result = callback(current.result, *args, **kw)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scrapy\\spiders\\__init__.py\", line 80, in parse\n",
      "    raise NotImplementedError('{}.parse callback is not defined'.format(self.__class__.__name__))\n",
      "NotImplementedError: medSpider.parse callback is not defined\n",
      "2019-09-11 15:00:13 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2019-09-11 15:00:13 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 474,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 4576,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'downloader/response_status_count/302': 1,\n",
      " 'elapsed_time_seconds': 0.721802,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2019, 9, 11, 19, 0, 13, 338225),\n",
      " 'log_count/DEBUG': 2,\n",
      " 'log_count/ERROR': 1,\n",
      " 'log_count/INFO': 10,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'spider_exceptions/NotImplementedError': 1,\n",
      " 'start_time': datetime.datetime(2019, 9, 11, 19, 0, 12, 616423)}\n",
      "2019-09-11 15:00:13 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "# Importing in each cell because of the kernel restarts.\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "class medSpider(scrapy.Spider):\n",
    "    # Naming the spider is important if you are running more than one spider of\n",
    "    # this class simultaneously.\n",
    "    name = \"med\"\n",
    "    \n",
    "    # URL(s) to start with.\n",
    "    start_urls = [\n",
    "        'https://eutils.ncbi.nlm.nih.gov/entrez/eutils',\n",
    "    ]\n",
    "\n",
    "    # What to do with the URL.  \n",
    "    #Example: Get the PubMed IDs (PMIDs) for articles about breast cancer published in Science in 2008\n",
    "    medfile = open(\"2008_BC_articles.xml\",\"w\")\n",
    "    call_text = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?api_key=9ad265d40cc5f7f7dcc6a51875f191806208&email=charlcap@aol.com&db=pubmed&term=science[journal]+AND+breast+cancer+AND+2008[pdat]&retmax=10&retmode=xml'\n",
    "    \n",
    "    medfile.write(call_text)\n",
    "    medfile.close()\n",
    "\n",
    "\n",
    "# Instantiate our crawler.\n",
    "process = CrawlerProcess()\n",
    "\n",
    "# Start the crawler with our spider.\n",
    "process.crawl(medSpider)\n",
    "process.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "medfile = pd.read_csv('medfile.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year Published</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>168.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2007.755952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.774709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1973.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2005.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2009.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2011.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2019.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Year Published\n",
       "count      168.000000\n",
       "mean      2007.755952\n",
       "std          5.774709\n",
       "min       1973.000000\n",
       "25%       2005.000000\n",
       "50%       2009.000000\n",
       "75%       2011.250000\n",
       "max       2019.000000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medfile.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method IndexOpsMixin.value_counts of 0      Health Professionals\n",
       "1      Health Professionals\n",
       "2      Health Professionals\n",
       "3       Health Professional\n",
       "4                 Consumers\n",
       "5                 Consumers\n",
       "6                 Consumers\n",
       "7                 Consumers\n",
       "8                 Consumers\n",
       "9                 Consumers\n",
       "10                Consumers\n",
       "11                Consumers\n",
       "12                Consumers\n",
       "13         Spanish language\n",
       "14                Consumers\n",
       "15         Spanish language\n",
       "16                Consumers\n",
       "17         Spanish language\n",
       "18         Spanish language\n",
       "19      Health Professional\n",
       "20                Consumers\n",
       "21                Consumers\n",
       "22     Health Professionals\n",
       "23         Spanish language\n",
       "24                Consumers\n",
       "25         Spanish language\n",
       "26     Health Professionals\n",
       "27                Consumers\n",
       "28         Spanish language\n",
       "29                Consumers\n",
       "               ...         \n",
       "138        Spanish language\n",
       "139               Consumers\n",
       "140     Health Professional\n",
       "141     Health Professional\n",
       "142               Consumers\n",
       "143               Consumers\n",
       "144               Consumers\n",
       "145               Consumers\n",
       "146               Consumers\n",
       "147               Consumers\n",
       "148               Consumers\n",
       "149               Consumers\n",
       "150               Consumers\n",
       "151               Consumers\n",
       "152     Health Professional\n",
       "153     Health Professional\n",
       "154               Consumers\n",
       "155     Health Professional\n",
       "156     Health Professional\n",
       "157               Consumers\n",
       "158               Consumers\n",
       "159        Spanish language\n",
       "160     Health Professional\n",
       "161     Health Professional\n",
       "162     Health Professional\n",
       "163     Health Professional\n",
       "164               Consumers\n",
       "165     Health Professional\n",
       "166                     NaN\n",
       "167                     NaN\n",
       "Name: Pub Type, Length: 168, dtype: object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medfile['Pub Type'].value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method IndexOpsMixin.value_counts of 0                              National Cancer Institute\n",
       "1                              National Cancer Institute\n",
       "2                              National Cancer Institute\n",
       "3                              National Cancer Institute\n",
       "4                              National Cancer Institute\n",
       "5                              National Cancer Institute\n",
       "6      National Institute of Dental and Craniofacial ...\n",
       "7      National Institute of Dental and Craniofacial ...\n",
       "8      National Institute of Dental and Craniofacial ...\n",
       "9      National Institute of Dental and Craniofacial ...\n",
       "10                             National Cancer Institute\n",
       "11                             National Cancer Institute\n",
       "12                             National Cancer Institute\n",
       "13                             National Cancer Institute\n",
       "14                             National Cancer Institute\n",
       "15                             National Cancer Institute\n",
       "16                             National Cancer Institute\n",
       "17     National Institute of Dental and Craniofacial ...\n",
       "18     National Institute of Dental and Craniofacial ...\n",
       "19                             National Cancer Institute\n",
       "20     National Institute of Dental and Craniofacial ...\n",
       "21     National Institute of Dental and Craniofacial ...\n",
       "22     National Institute of Environmental Health Sci...\n",
       "23                             National Cancer Institute\n",
       "24                             National Cancer Institute\n",
       "25                             National Cancer Institute\n",
       "26     National Institute of Dental and Craniofacial ...\n",
       "27                             National Cancer Institute\n",
       "28                             National Cancer Institute\n",
       "29     National Institute of Arthritis and Musculoske...\n",
       "                             ...                        \n",
       "138                            National Cancer Institute\n",
       "139                            National Cancer Institute\n",
       "140                            National Cancer Institute\n",
       "141                            National Cancer Institute\n",
       "142                            National Cancer Institute\n",
       "143                            National Cancer Institute\n",
       "144                            National Cancer Institute\n",
       "145                            National Cancer Institute\n",
       "146                            National Cancer Institute\n",
       "147                            National Cancer Institute\n",
       "148                            National Cancer Institute\n",
       "149                            National Cancer Institute\n",
       "150                            National Cancer Institute\n",
       "151                            National Cancer Institute\n",
       "152                            National Cancer Institute\n",
       "153                            National Cancer Institute\n",
       "154                            National Cancer Institute\n",
       "155                            National Cancer Institute\n",
       "156                            National Cancer Institute\n",
       "157                            National Cancer Institute\n",
       "158                            National Cancer Institute\n",
       "159                            National Cancer Institute\n",
       "160                            National Cancer Institute\n",
       "161                            National Cancer Institute\n",
       "162                            National Cancer Institute\n",
       "163                            National Cancer Institute\n",
       "164                            National Cancer Institute\n",
       "165                            National Cancer Institute\n",
       "166                 John E. Fogarty International Center\n",
       "167    National Institute of Allergy and Infectious D...\n",
       "Name: IC, Length: 168, dtype: object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medfile['IC'].value_counts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
